{"cells":[{"cell_type":"markdown","metadata":{},"source":["# OpenAI Gym: CSPB-3202 Introduction to Artificial Intelligence\n","\n","### Final Semester Project\n","\n","### Author: Micah Simmerman \n","\n","### Date: December 07, 2023"]},{"cell_type":"markdown","metadata":{},"source":["This end-of-semester project will explore agent and environmental design for Reinforcement Learning (RL) design using the pythonic OpenAI Gym library suite of tools.  This project and its' accompanying presentation(s) will seek to explore the finer-point details of RL agents and reinforcement learning environment design. We will first build models with limited degree-of-freedom (DOF) complexity, and then gradually explore the design considerations of more complicated agents and environments. \n","\n","Please read all inline documentation as sources will often be cited there. "]},{"attachments":{},"cell_type":"markdown","metadata":{"colab_type":"text","id":"ReRFzrTl77hl"},"source":["Project Itinerary:\n","\n","    1.) Cartpole: starting example\n","    2.) Lunar Lander\n","    3.) Taxi Cab\n","    4.) HalfCheetah"]},{"cell_type":"markdown","metadata":{},"source":["### Import Gym, Gymnasium, IPython display (in place of Box2D),"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{},"colab_type":"code","id":"x_Jq7g1-77hm"},"outputs":[{"ename":"FileNotFoundError","evalue":"Could not find module 'C:\\Users\\jmica\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\atari_py\\ale_interface\\ale_c.dll' (or one of its dependencies). Try using the full path with constructor syntax.","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[1;32mc:\\Users\\jmica\\Desktop\\AI_Semester_Project\\semester_project_notebook.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jmica/Desktop/AI_Semester_Project/semester_project_notebook.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgymnasium\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mgym\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jmica/Desktop/AI_Semester_Project/semester_project_notebook.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# import gym\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jmica/Desktop/AI_Semester_Project/semester_project_notebook.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mIPython\u001b[39;00m \u001b[39mimport\u001b[39;00m display  \u001b[39m# alternative to Box2D\u001b[39;00m\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gymnasium\\__init__.py:12\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgymnasium\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m      5\u001b[0m     Env,\n\u001b[0;32m      6\u001b[0m     Wrapper,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     RewardWrapper,\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgymnasium\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspaces\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspace\u001b[39;00m \u001b[39mimport\u001b[39;00m Space\n\u001b[1;32m---> 12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgymnasium\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menvs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mregistration\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     13\u001b[0m     make,\n\u001b[0;32m     14\u001b[0m     spec,\n\u001b[0;32m     15\u001b[0m     register,\n\u001b[0;32m     16\u001b[0m     registry,\n\u001b[0;32m     17\u001b[0m     pprint_registry,\n\u001b[0;32m     18\u001b[0m     make_vec,\n\u001b[0;32m     19\u001b[0m     register_envs,\n\u001b[0;32m     20\u001b[0m )\n\u001b[0;32m     22\u001b[0m \u001b[39m# necessary for `envs.__init__` which registers all gymnasium environments and loads plugins\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgymnasium\u001b[39;00m \u001b[39mimport\u001b[39;00m envs\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gymnasium\\envs\\__init__.py:387\u001b[0m\n\u001b[0;32m    383\u001b[0m register(\u001b[39mid\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGymV26Environment-v0\u001b[39m\u001b[39m\"\u001b[39m, entry_point\u001b[39m=\u001b[39m_raise_shimmy_error)\n\u001b[0;32m    386\u001b[0m \u001b[39m# Hook to load plugins from entry points\u001b[39;00m\n\u001b[1;32m--> 387\u001b[0m load_plugin_envs()\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gymnasium\\envs\\registration.py:592\u001b[0m, in \u001b[0;36mload_plugin_envs\u001b[1;34m(entry_point)\u001b[0m\n\u001b[0;32m    586\u001b[0m         logger\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    587\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe environment namespace magic key `\u001b[39m\u001b[39m{\u001b[39;00mplugin\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m` is unsupported. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    588\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mTo register an environment at the root namespace you should specify the `__root__` namespace.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    589\u001b[0m         )\n\u001b[0;32m    591\u001b[0m \u001b[39mwith\u001b[39;00m context:\n\u001b[1;32m--> 592\u001b[0m     fn \u001b[39m=\u001b[39m plugin\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m    593\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    594\u001b[0m         fn()\n","File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\importlib\\metadata\\__init__.py:171\u001b[0m, in \u001b[0;36mEntryPoint.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Load the entry point from its definition. If only a module\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[39mis indicated by the value, return that module. Otherwise,\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[39mreturn the named object.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    170\u001b[0m match \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpattern\u001b[39m.\u001b[39mmatch(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue)\n\u001b[1;32m--> 171\u001b[0m module \u001b[39m=\u001b[39m import_module(match\u001b[39m.\u001b[39;49mgroup(\u001b[39m'\u001b[39;49m\u001b[39mmodule\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m    172\u001b[0m attrs \u001b[39m=\u001b[39m \u001b[39mfilter\u001b[39m(\u001b[39mNone\u001b[39;00m, (match\u001b[39m.\u001b[39mgroup(\u001b[39m'\u001b[39m\u001b[39mattr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m    173\u001b[0m \u001b[39mreturn\u001b[39;00m functools\u001b[39m.\u001b[39mreduce(\u001b[39mgetattr\u001b[39m, attrs, module)\n","File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\shimmy\\__init__.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Any\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mshimmy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdm_lab_compatibility\u001b[39;00m \u001b[39mimport\u001b[39;00m DmLabCompatibilityV0\n\u001b[1;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mshimmy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mopenai_gym_compatibility\u001b[39;00m \u001b[39mimport\u001b[39;00m GymV21CompatibilityV0, GymV26CompatibilityV0\n\u001b[0;32m      9\u001b[0m __version__ \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m0.2.1\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mNotInstallClass\u001b[39;00m:\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\shimmy\\openai_gym_compatibility.py:35\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mtyping_extensions\u001b[39;00m \u001b[39mimport\u001b[39;00m Protocol, runtime_checkable\n\u001b[0;32m     34\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 35\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mgym\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mgym\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwrappers\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gym\\__init__.py:12\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgym\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m      5\u001b[0m     Env,\n\u001b[0;32m      6\u001b[0m     Wrapper,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     RewardWrapper,\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgym\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspaces\u001b[39;00m \u001b[39mimport\u001b[39;00m Space\n\u001b[1;32m---> 12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgym\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menvs\u001b[39;00m \u001b[39mimport\u001b[39;00m make, spec, register\n\u001b[0;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgym\u001b[39;00m \u001b[39mimport\u001b[39;00m logger\n\u001b[0;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgym\u001b[39;00m \u001b[39mimport\u001b[39;00m vector\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gym\\envs\\__init__.py:10\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgym\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menvs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mregistration\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     registry,\n\u001b[0;32m      3\u001b[0m     register,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m     load_env_plugins \u001b[39mas\u001b[39;00m _load_env_plugins,\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      9\u001b[0m \u001b[39m# Hook to load plugins from entry points\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m _load_env_plugins()\n\u001b[0;32m     13\u001b[0m \u001b[39m# Classic\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m# ----------------------------------------\u001b[39;00m\n\u001b[0;32m     16\u001b[0m register(\n\u001b[0;32m     17\u001b[0m     \u001b[39mid\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCartPole-v0\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     18\u001b[0m     entry_point\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgym.envs.classic_control:CartPoleEnv\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     19\u001b[0m     max_episode_steps\u001b[39m=\u001b[39m\u001b[39m200\u001b[39m,\n\u001b[0;32m     20\u001b[0m     reward_threshold\u001b[39m=\u001b[39m\u001b[39m195.0\u001b[39m,\n\u001b[0;32m     21\u001b[0m )\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gym\\envs\\registration.py:725\u001b[0m, in \u001b[0;36mload_env_plugins\u001b[1;34m(entry_point)\u001b[0m\n\u001b[0;32m    718\u001b[0m         logger\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    719\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe environment namespace magic key `\u001b[39m\u001b[39m{\u001b[39;00mplugin\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m` is unsupported. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    720\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mTo register an environment at the root namespace you should specify \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    721\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mthe `__root__` namespace.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    722\u001b[0m         )\n\u001b[0;32m    724\u001b[0m \u001b[39mwith\u001b[39;00m context:\n\u001b[1;32m--> 725\u001b[0m     fn \u001b[39m=\u001b[39m plugin\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m    726\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    727\u001b[0m         fn()\n","File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\importlib\\metadata\\__init__.py:171\u001b[0m, in \u001b[0;36mEntryPoint.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Load the entry point from its definition. If only a module\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[39mis indicated by the value, return that module. Otherwise,\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[39mreturn the named object.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    170\u001b[0m match \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpattern\u001b[39m.\u001b[39mmatch(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue)\n\u001b[1;32m--> 171\u001b[0m module \u001b[39m=\u001b[39m import_module(match\u001b[39m.\u001b[39;49mgroup(\u001b[39m'\u001b[39;49m\u001b[39mmodule\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m    172\u001b[0m attrs \u001b[39m=\u001b[39m \u001b[39mfilter\u001b[39m(\u001b[39mNone\u001b[39;00m, (match\u001b[39m.\u001b[39mgroup(\u001b[39m'\u001b[39m\u001b[39mattr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m    173\u001b[0m \u001b[39mreturn\u001b[39;00m functools\u001b[39m.\u001b[39mreduce(\u001b[39mgetattr\u001b[39m, attrs, module)\n","File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\ale_py\\gym.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcollections\u001b[39;00m \u001b[39mimport\u001b[39;00m defaultdict, namedtuple\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgym\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menvs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mregistration\u001b[39;00m \u001b[39mimport\u001b[39;00m register\n\u001b[1;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39male_py\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mroms\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m rom_name_to_id, rom_id_to_name\n\u001b[0;32m      8\u001b[0m GymFlavour \u001b[39m=\u001b[39m namedtuple(\u001b[39m\"\u001b[39m\u001b[39mGymFlavour\u001b[39m\u001b[39m\"\u001b[39m, [\u001b[39m\"\u001b[39m\u001b[39msuffix\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39menv_kwargs\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mkwargs\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m      9\u001b[0m GymConfig \u001b[39m=\u001b[39m namedtuple(\u001b[39m\"\u001b[39m\u001b[39mGymConfig\u001b[39m\u001b[39m\"\u001b[39m, [\u001b[39m\"\u001b[39m\u001b[39mversion\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39menv_kwargs\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mflavours\u001b[39m\u001b[39m\"\u001b[39m])\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\ale_py\\roms\\__init__.py:89\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[39mreturn\u001b[39;00m roms\n\u001b[0;32m     88\u001b[0m \u001b[39m# Resolve all ROMs\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m ROMS \u001b[39m=\u001b[39m resolve_roms()\n\u001b[0;32m     90\u001b[0m __all__ \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ROMS\u001b[39m.\u001b[39mkeys())\n\u001b[0;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__dir__\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\ale_py\\roms\\__init__.py:40\u001b[0m, in \u001b[0;36mresolve_roms\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39mfor\u001b[39;00m package \u001b[39min\u001b[39;00m ROM_PLUGINS:\n\u001b[0;32m     38\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m         \u001b[39m# Resolve supported / unsupported roms\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m         supported, unsupported \u001b[39m=\u001b[39m package\u001b[39m.\u001b[39;49mresolve()\n\u001b[0;32m     42\u001b[0m         \u001b[39m# We'll now get the update delta. The reason for this is two fold:\u001b[39;00m\n\u001b[0;32m     43\u001b[0m         \u001b[39m#     1) We should only display atari-py deprecation when it would have\u001b[39;00m\n\u001b[0;32m     44\u001b[0m         \u001b[39m#        imported ROMs.\u001b[39;00m\n\u001b[0;32m     45\u001b[0m         \u001b[39m#     2) ROM priority holds. When you import ROMs they'll all come from\u001b[39;00m\n\u001b[0;32m     46\u001b[0m         \u001b[39m#        a single source of truth.\u001b[39;00m\n\u001b[0;32m     47\u001b[0m         \u001b[39m#\u001b[39;00m\n\u001b[0;32m     48\u001b[0m         roms_delta_keys \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[0;32m     49\u001b[0m             \u001b[39mfilter\u001b[39m(\u001b[39mlambda\u001b[39;00m rom: rom \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m roms, supported\u001b[39m.\u001b[39mkeys())\n\u001b[0;32m     50\u001b[0m         )\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\ale_py\\roms\\utils.py:60\u001b[0m, in \u001b[0;36mSupportedPackage.resolve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     56\u001b[0m unsupported: List[pathlib\u001b[39m.\u001b[39mPath] \u001b[39m=\u001b[39m []\n\u001b[0;32m     58\u001b[0m \u001b[39m# Iterate over all ROMs in the specified package\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[39mfor\u001b[39;00m resource \u001b[39min\u001b[39;00m \u001b[39mfilter\u001b[39m(\n\u001b[1;32m---> 60\u001b[0m     \u001b[39mlambda\u001b[39;00m file: file\u001b[39m.\u001b[39msuffix \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.bin\u001b[39m\u001b[39m\"\u001b[39m, resources\u001b[39m.\u001b[39;49mfiles(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpackage)\u001b[39m.\u001b[39miterdir()\n\u001b[0;32m     61\u001b[0m ):\n\u001b[0;32m     62\u001b[0m     resolved \u001b[39m=\u001b[39m resource\u001b[39m.\u001b[39mresolve()\n\u001b[0;32m     63\u001b[0m     rom \u001b[39m=\u001b[39m ALEInterface\u001b[39m.\u001b[39misSupportedROM(resolved)\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\importlib_resources\\_common.py:46\u001b[0m, in \u001b[0;36mpackage_to_anchor.<locals>.wrapper\u001b[1;34m(anchor, package)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39melif\u001b[39;00m anchor \u001b[39mis\u001b[39;00m undefined:\n\u001b[0;32m     45\u001b[0m     \u001b[39mreturn\u001b[39;00m func()\n\u001b[1;32m---> 46\u001b[0m \u001b[39mreturn\u001b[39;00m func(anchor)\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\importlib_resources\\_common.py:56\u001b[0m, in \u001b[0;36mfiles\u001b[1;34m(anchor)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39m@package_to_anchor\u001b[39m\n\u001b[0;32m     52\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfiles\u001b[39m(anchor: Optional[Anchor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Traversable:\n\u001b[0;32m     53\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[39m    Get a Traversable resource for an anchor.\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m     \u001b[39mreturn\u001b[39;00m from_package(resolve(anchor))\n","File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\functools.py:889\u001b[0m, in \u001b[0;36msingledispatch.<locals>.wrapper\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    885\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[0;32m    886\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mfuncname\u001b[39m}\u001b[39;00m\u001b[39m requires at least \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    887\u001b[0m                     \u001b[39m'\u001b[39m\u001b[39m1 positional argument\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 889\u001b[0m \u001b[39mreturn\u001b[39;00m dispatch(args[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\importlib_resources\\_common.py:82\u001b[0m, in \u001b[0;36m_\u001b[1;34m(cand)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[39m@resolve\u001b[39m\u001b[39m.\u001b[39mregister\n\u001b[0;32m     81\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_\u001b[39m(cand: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m types\u001b[39m.\u001b[39mModuleType:\n\u001b[1;32m---> 82\u001b[0m     \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39;49mimport_module(cand)\n","File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\atari_py\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39male_python_interface\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mgames\u001b[39;00m \u001b[39mimport\u001b[39;00m get_game_path, list_games\n\u001b[0;32m      4\u001b[0m \u001b[39m# default to only logging errors\u001b[39;00m\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\atari_py\\ale_python_interface.py:17\u001b[0m\n\u001b[0;32m     14\u001b[0m     ale_lib \u001b[39m=\u001b[39m cdll\u001b[39m.\u001b[39mLoadLibrary(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mdirname(\u001b[39m__file__\u001b[39m),\n\u001b[0;32m     15\u001b[0m                                             \u001b[39m'\u001b[39m\u001b[39male_interface/libale_c.so\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m     16\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 17\u001b[0m     ale_lib \u001b[39m=\u001b[39m cdll\u001b[39m.\u001b[39;49mLoadLibrary(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mdirname(\u001b[39m__file__\u001b[39;49m),\n\u001b[0;32m     18\u001b[0m                                             \u001b[39m'\u001b[39;49m\u001b[39male_interface/ale_c.dll\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m     20\u001b[0m ale_lib\u001b[39m.\u001b[39mALE_new\u001b[39m.\u001b[39margtypes \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     21\u001b[0m ale_lib\u001b[39m.\u001b[39mALE_new\u001b[39m.\u001b[39mrestype \u001b[39m=\u001b[39m c_void_p\n","File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\ctypes\\__init__.py:452\u001b[0m, in \u001b[0;36mLibraryLoader.LoadLibrary\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mLoadLibrary\u001b[39m(\u001b[39mself\u001b[39m, name):\n\u001b[1;32m--> 452\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dlltype(name)\n","File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\ctypes\\__init__.py:374\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[1;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_FuncPtr \u001b[39m=\u001b[39m _FuncPtr\n\u001b[0;32m    373\u001b[0m \u001b[39mif\u001b[39;00m handle \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 374\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle \u001b[39m=\u001b[39m _dlopen(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name, mode)\n\u001b[0;32m    375\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    376\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle \u001b[39m=\u001b[39m handle\n","\u001b[1;31mFileNotFoundError\u001b[0m: Could not find module 'C:\\Users\\jmica\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\atari_py\\ale_interface\\ale_c.dll' (or one of its dependencies). Try using the full path with constructor syntax."]}],"source":["import gymnasium as gym\n","# import gym\n","from IPython import display  # alternative to Box2D\n","# from gym import Box2D\n","\n","import warnings\n","from collections import namedtuple\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","partition = namedtuple(\"partition\", [\"type\", \"subtype\"])\n","gym.__version__\n","e_cartpole = gym.make('CartPole-v1')   # coinstruct the environment 'e'\n","print(\"result of gym.make('CartPole-v1'); \", e_cartpole)  # shows the environment wrapper structure.\n","obs = e_cartpole.reset()  # reset cartpole environment\n","\n","print(\"Environment action space: \", e_cartpole.action_space) # discrete action choices: [left, right]\n","print(\"~~~~~~~~~~~~~~~~~~~~~~~~\")\n","print(\"Observed transitions: \", obs)\n","\n","# The action space is defined in gym.spaces\n","from gym.spaces.discrete import Discrete\n","d = Discrete(2) #the Discrete is a class that has methods .sample and .contains\n","[d.sample() for x in range(10)] #sample generates an action output\n","\n","print(d.contains(0), d.contains(2)) #with .contains method, you can check whether an integer is a valid action\n","print(e_cartpole.observation_space) #returns Box class, which represents n-dim tensor\n","e_cartpole.step(1)\n","print()\n","\n","e_cartpole.step(0) \n","# see https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py\n","# force = self.force_mag if action==1 else -self.force_mag"]},{"cell_type":"markdown","metadata":{},"source":["### Import Packages and Get Ready to use Torch"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"AttributeError","evalue":"partially initialized module 'gym' has no attribute 'core' (most likely due to a circular import)","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[1;32mc:\\Users\\jmica\\Desktop\\AI_Semester_Project\\semester_project_notebook.ipynb Cell 7\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jmica/Desktop/AI_Semester_Project/semester_project_notebook.ipynb#X43sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm  \u001b[39m# provides progress bar functionality\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jmica/Desktop/AI_Semester_Project/semester_project_notebook.ipynb#X43sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcopy\u001b[39;00m \u001b[39mimport\u001b[39;00m deepcopy  \u001b[39m# enables deep and shallow copying of python class objects\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jmica/Desktop/AI_Semester_Project/semester_project_notebook.ipynb#X43sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgym\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwrappers\u001b[39;00m \u001b[39mimport\u001b[39;00m RecordVideo  \u001b[39m# \u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jmica/Desktop/AI_Semester_Project/semester_project_notebook.ipynb#X43sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jmica/Desktop/AI_Semester_Project/semester_project_notebook.ipynb#X43sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m \n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gym\\__init__.py:15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgym\u001b[39;00m \u001b[39mimport\u001b[39;00m logger\n\u001b[0;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgym\u001b[39;00m \u001b[39mimport\u001b[39;00m vector\n\u001b[1;32m---> 15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgym\u001b[39;00m \u001b[39mimport\u001b[39;00m wrappers\n\u001b[0;32m     16\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m     19\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mEnv\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mSpace\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mWrapper\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmake\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mspec\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mregister\u001b[39m\u001b[39m\"\u001b[39m]\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gym\\wrappers\\__init__.py:16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgym\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwrappers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclip_action\u001b[39;00m \u001b[39mimport\u001b[39;00m ClipAction\n\u001b[0;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgym\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwrappers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrecord_episode_statistics\u001b[39;00m \u001b[39mimport\u001b[39;00m RecordEpisodeStatistics\n\u001b[1;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgym\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwrappers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnormalize\u001b[39;00m \u001b[39mimport\u001b[39;00m NormalizeObservation, NormalizeReward\n\u001b[0;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgym\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwrappers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrecord_video\u001b[39;00m \u001b[39mimport\u001b[39;00m RecordVideo, capped_cubic_video_schedule\n\u001b[0;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgym\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwrappers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39morder_enforcing\u001b[39;00m \u001b[39mimport\u001b[39;00m OrderEnforcing\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gym\\wrappers\\normalize.py:43\u001b[0m\n\u001b[0;32m     38\u001b[0m     new_count \u001b[39m=\u001b[39m tot_count\n\u001b[0;32m     40\u001b[0m     \u001b[39mreturn\u001b[39;00m new_mean, new_var, new_count\n\u001b[1;32m---> 43\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mNormalizeObservation\u001b[39;00m(gym\u001b[39m.\u001b[39;49mcore\u001b[39m.\u001b[39mWrapper):\n\u001b[0;32m     44\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m     45\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[0;32m     46\u001b[0m         env,\n\u001b[0;32m     47\u001b[0m         epsilon\u001b[39m=\u001b[39m\u001b[39m1e-8\u001b[39m,\n\u001b[0;32m     48\u001b[0m     ):\n\u001b[0;32m     49\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(env)\n","\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'gym' has no attribute 'core' (most likely due to a circular import)"]}],"source":["import numpy as np   # for complex calculations\n","import matplotlib.pyplot as plt   # for graphs and plots\n","from tqdm import tqdm  # provides progress bar functionality\n","from copy import deepcopy  # enables deep and shallow copying of python class objects\n","\n","from gym.wrappers import RecordVideo  # \n","from utils import *\n","\n","import torch \n","from torch import nn\n","import torch.nn.functional as F\n","from torch import optim \n","\n","\n","%matplotlib inline\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(\"Hardware used: \", device)\n","# torch.cuda.is_available()  # NEED TO INSTALL CUDA DRIVERS TO ABSTRACT THE GPU"]},{"cell_type":"markdown","metadata":{},"source":["The following cells produce an agent that can navigate the different types of OpenAI Gym environments that we will be creating in this notebook. We can also use any of the approaches shown below to complete the assignment."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"oTjeN5wu77iO"},"source":["## Making an agent\n","Since the `gym` provides us the environment, it's our job to make an agent (policy) that can interact with the environemnt. Here is an example of a random agent."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","executionInfo":{"elapsed":1187,"status":"ok","timestamp":1587754481810,"user":{"displayName":"Ketan Ramesh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjfwRjj49F2JlDyKTfXmTOoBNu-A4deHVrB1vg0Qw=s64","userId":"03181076370714837334"},"user_tz":360},"id":"9w2CUN6I77iO","outputId":"2adfbef0-d104-46f6-8c70-d103bc2bb94d"},"outputs":[],"source":["import gymnasium as gym\n","# for more information about OpenAI Gym Wrappers visit: https://gymnasium.farama.org/api/wrappers/observation_wrappers/\n","\n","env = gym.make(\"CartPole-v1\")  # creates a cartpole v1 gym env class object\n","\n","total_reward = 0.0  # initiate total rewards var\n","total_steps = 0  # initiate step counter\n","obs = env.reset()  # reset the env to an initial internal state, returns observation and info. \n","\n","while True:  # continuous while loop\n","    action = env.action_space.sample() #.sample method gives a random action sample\n","    obs, reward, done, _, _ = env.step(action)  # \"update the environment with actions returning the next agent observation\" - google\n","    total_reward += reward  # increment the reward acc var by the earned amount. \n","    total_steps += 1  # increment step counter.\n","    if done:  # not sure where \"done\" gets set and updated. \n","        break  # discontinue while loop.\n","\n","print(\"Episode done in %d steps, total reward %.2f\" % (total_steps, total_reward))"]},{"attachments":{},"cell_type":"markdown","metadata":{"colab_type":"text","id":"4Yiqkd6V77iR"},"source":["## Monitoring the agent"]},{"cell_type":"markdown","metadata":{},"source":["Sources: \n","\n","    https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html\n","    https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.clf.html\n","    https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.figure.html\n","    https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.title.html\n","    https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.axis.html\n","\n","The following cell defines a custom function that allows us to monitor the agent's gameplay. The cell also invokes matplotlib and IPython libraries used to render the display."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"Z4bpB3RO77iU"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","from IPython import display  # IMPORTANT!: this may be a good alternative to the failed \"BOx2D\" library install.\n","\n","# The \"show_state\" function is used to render visual information about the agent's interaction with the environment\n","# 'show_state' is a user defined function that controls the visual rendering of the 2D Gym environment plus agent\n","def show_state(env, step=0, info=\"\"):  # user defined function handling environment, step count initialization (?), empty \"info\" acc string variable.\n","    plt.figure(3)  # \"num\" is a int, str, Figure, or SubFigure and an optional variable for the plt library. \"figure\" creates a new figure, oc.\n","    plt.clf()  # clears the current figure.\n","    plt.imshow(env.render())  # Displays data (the agents given environment, in this case) as an image on a regular 2D roster.\n","    plt.title(\"%s | Step: %d %s\" % (env.spec.id,step, info))  # title(label, fontdict=None, loc='center', pad=None, **kwargs)\n","    plt.axis('off')  # A \"Convenience method to get or set some axis properties.\" Example: xmin, xmax, ymin, ymax = axis()\n","\n","    display.clear_output(wait=True)  # removes current output of the cell, enables collection of the latest details\n","    display.display(plt.gcf())  # plt.gcf collects the current figure. If there is no current figure on the pyplot figure stack, a new (blank one) is created using figure()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":528},"colab_type":"code","executionInfo":{"elapsed":10205,"status":"ok","timestamp":1587754490872,"user":{"displayName":"Ketan Ramesh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjfwRjj49F2JlDyKTfXmTOoBNu-A4deHVrB1vg0Qw=s64","userId":"03181076370714837334"},"user_tz":360},"id":"7akosTK_77iV","outputId":"b3650d5c-51a2-4bc4-fb25-4db8290f29f7"},"outputs":[],"source":["# We follow a similar pattern of each time: \n","# environment creation => initiate 'reward_sum' and 'step_count' => (re)set the environment => gaurded while loop => display render\n","# \n","env = gym.make(\"CartPole-v1\", render_mode='rgb_array')  # generate the gym environment by invoking the .make class function\n","total_reward = 0.0  # initiate reward sum\n","total_steps = 0  # \n","obs = env.reset()  # \n","\n","while True:  # initiate an infinity while loop controlled by agent decisions and resulting game outcomes.\n","    action = env.action_space.sample()\n","    obs, reward, done, _, _ = env.step(action)\n","    total_reward += reward\n","    total_steps += 1\n","    show_state(env,total_steps)\n","    if done:  # done == True when Gym game environment reaches a terminating state\n","        break  # then we break out of the loop\n","\n","print(\"Episode done in %d steps, total reward %.2f\" % (total_steps, total_reward))\n","env.close()\n","env.env.close()"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"1IMqU34s77iX"},"source":["## Environment wrappers\n","`Env` offers a `Wrapper` class that allow you to modify settings in the `Env` class. `Wrapper` has three subclasses `RewardWrapper`, `ObservationWrapper` and `ActionWrapper`.\n","Let's take an example of an agent that takes a random action e.g. 10% of the time. We'll write a class that overrides `Env`'s action usging `ActionWrapper`. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"4A336uTY77iX"},"outputs":[],"source":["import gymnasium as gym\n","import random\n","\n","# From doc source: \n","# \"If you would like to apply a function to the action before passing it to the base environment, you can simply \n","# inherit from ActionWrapper and overwrite the method action() to implement that transformation.\"\n","# RandomEpsilonWrapper creates a random agent for the cartpole environment that inherits from the gym.ActionWrapper base class. \n","class RandomEpsilonWrapper(gym.ActionWrapper):  # gym.ActionWrapper is a Gym Base Class that can modify the action before env.step()\n","    def __init__(self, env, epsilon=0.1):  # initiates the RandomEpsilonWrapper inheritence object\n","        super(RandomEpsilonWrapper, self).__init__(env)  # RandomEpsilonWrapper is an epsilon-greedy method\n","        self.epsilon = epsilon  # set epsilon self value\n","\n","\n","    def action(self, action):  # overrides base class definition, takes action and acts randomly according to self.epsilon\n","        if random.random() < self.epsilon:  # make a random stochastic selection and compare it greedily to reduce epsilon\n","            print(\"Random!\")  # disregard the policy-informed action in exchange for a randomly-selected action \n","            return self.env.action_space.sample()  # agent has \"chosen\" to act randomly (controlled by epsilon) (exploration)\n","        else:\n","            print(\"Policy\")  # agent made a decision informed by the current policy (exploitation)\n","        return action"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":289},"colab_type":"code","executionInfo":{"elapsed":10190,"status":"ok","timestamp":1587754490873,"user":{"displayName":"Ketan Ramesh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjfwRjj49F2JlDyKTfXmTOoBNu-A4deHVrB1vg0Qw=s64","userId":"03181076370714837334"},"user_tz":360},"id":"423lULmx77iY","outputId":"91166488-ed8f-4e82-e9aa-667db54edc27"},"outputs":[],"source":["env = RandomEpsilonWrapper(gym.make(\"CartPole-v1\"),epsilon=0.5)  # modify the environment with RandomEpsilonWrapper\n","obs = env.reset()  # \n","total_reward = 0.0  # \n","\n","while True:\n","    obs, reward, done, _, _= env.step(0) #we have a fixed policy of going to left (action = 0) only\n","    total_reward += reward\n","    #show_state(env) #with this line uncommented, you can also monitor\n","    if done:\n","        break\n","\n","print(\"Reward got: %.2f\" % total_reward)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"VjUnhe6Q77ia"},"source":["## Implementing simple  reflex agent\n","Below is the rule defined in the [Cartpole environment](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py)\n","```\n","   Observation: \n","        Type: Box(4)\n","        Num\tObservation                 Min         Max\n","        0\tCart Position             -4.8            4.8\n","        1\tCart Velocity             -Inf            Inf\n","        2\tPole Angle                 -24 deg        24 deg\n","        3\tPole Velocity At Tip      -Inf            Inf\n","        \n","    Actions:\n","        Type: Discrete(2)\n","        Num\tAction\n","        0\tPush cart to the left\n","        1\tPush cart to the right\n","        \n","        Note: The amount the velocity that is reduced or increased is not fixed; it depends on the angle the pole is pointing. This is because the center of gravity of the pole increases the amount of energy needed to move the cart underneath it\n","    Reward:\n","        Reward is 1 for every step taken, including the termination step\n","    Starting State:\n","        All observations are assigned a uniform random value in [-0.05..0.05]\n","    Episode Termination:\n","        Pole Angle is more than 12 degrees\n","        Cart Position is more than 2.4 (center of the cart reaches the edge of the display)\n","        Episode length is greater than 200\n","        Solved Requirements\n","        Considered solved when the average reward is greater than or equal to 195.0 over 100 consecutive trials.\n","```\n","\n","Let's make an agent that act deterministically. Let's say the agent can sense the velocity of the cart vc and the velocity of the pole vp. It will continue going to the same direction until it senses the pole starts falling toward the oposite direction of it moving (Simple reflex agent).\n","\n","Here, we create an Agent class first, which runs single episode and can run episodes n times so that we can do some stats. Then we'll create subclasses using inheritance (below examples do not use `super` as we used in the wrapper example above)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{},"colab_type":"code","id":"uqFwTHi677ia"},"outputs":[{"ename":"AttributeError","evalue":"partially initialized module 'gym' has no attribute 'core' (most likely due to a circular import)","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[1;32mc:\\Users\\jmica\\Desktop\\AI_Semester_Project\\semester_project_notebook.ipynb Cell 19\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jmica/Desktop/AI_Semester_Project/semester_project_notebook.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# deterministic rule\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jmica/Desktop/AI_Semester_Project/semester_project_notebook.ipynb#X33sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# first choose random action\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jmica/Desktop/AI_Semester_Project/semester_project_notebook.ipynb#X33sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# monitor vc and vp while keep going to the same direction\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jmica/Desktop/AI_Semester_Project/semester_project_notebook.ipynb#X33sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# if the sign of the two velocities are different, flip the direction (action)\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jmica/Desktop/AI_Semester_Project/semester_project_notebook.ipynb#X33sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgymnasium\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mgym\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jmica/Desktop/AI_Semester_Project/semester_project_notebook.ipynb#X33sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# import gym\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jmica/Desktop/AI_Semester_Project/semester_project_notebook.ipynb#X33sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gymnasium\\__init__.py:23\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgymnasium\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menvs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mregistration\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     13\u001b[0m     make,\n\u001b[0;32m     14\u001b[0m     spec,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m     register_envs,\n\u001b[0;32m     20\u001b[0m )\n\u001b[0;32m     22\u001b[0m \u001b[39m# necessary for `envs.__init__` which registers all gymnasium environments and loads plugins\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgymnasium\u001b[39;00m \u001b[39mimport\u001b[39;00m envs\n\u001b[0;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgymnasium\u001b[39;00m \u001b[39mimport\u001b[39;00m spaces, utils, vector, wrappers, error, logger\n\u001b[0;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgymnasium\u001b[39;00m \u001b[39mimport\u001b[39;00m experimental\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gymnasium\\envs\\__init__.py:387\u001b[0m\n\u001b[0;32m    383\u001b[0m register(\u001b[39mid\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGymV26Environment-v0\u001b[39m\u001b[39m\"\u001b[39m, entry_point\u001b[39m=\u001b[39m_raise_shimmy_error)\n\u001b[0;32m    386\u001b[0m \u001b[39m# Hook to load plugins from entry points\u001b[39;00m\n\u001b[1;32m--> 387\u001b[0m load_plugin_envs()\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gymnasium\\envs\\registration.py:592\u001b[0m, in \u001b[0;36mload_plugin_envs\u001b[1;34m(entry_point)\u001b[0m\n\u001b[0;32m    586\u001b[0m         logger\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    587\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe environment namespace magic key `\u001b[39m\u001b[39m{\u001b[39;00mplugin\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m` is unsupported. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    588\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mTo register an environment at the root namespace you should specify the `__root__` namespace.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    589\u001b[0m         )\n\u001b[0;32m    591\u001b[0m \u001b[39mwith\u001b[39;00m context:\n\u001b[1;32m--> 592\u001b[0m     fn \u001b[39m=\u001b[39m plugin\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m    593\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    594\u001b[0m         fn()\n","File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\importlib\\metadata\\__init__.py:171\u001b[0m, in \u001b[0;36mEntryPoint.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Load the entry point from its definition. If only a module\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[39mis indicated by the value, return that module. Otherwise,\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[39mreturn the named object.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    170\u001b[0m match \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpattern\u001b[39m.\u001b[39mmatch(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue)\n\u001b[1;32m--> 171\u001b[0m module \u001b[39m=\u001b[39m import_module(match\u001b[39m.\u001b[39;49mgroup(\u001b[39m'\u001b[39;49m\u001b[39mmodule\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m    172\u001b[0m attrs \u001b[39m=\u001b[39m \u001b[39mfilter\u001b[39m(\u001b[39mNone\u001b[39;00m, (match\u001b[39m.\u001b[39mgroup(\u001b[39m'\u001b[39m\u001b[39mattr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m    173\u001b[0m \u001b[39mreturn\u001b[39;00m functools\u001b[39m.\u001b[39mreduce(\u001b[39mgetattr\u001b[39m, attrs, module)\n","File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\shimmy\\__init__.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Any\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mshimmy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdm_lab_compatibility\u001b[39;00m \u001b[39mimport\u001b[39;00m DmLabCompatibilityV0\n\u001b[1;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mshimmy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mopenai_gym_compatibility\u001b[39;00m \u001b[39mimport\u001b[39;00m GymV21CompatibilityV0, GymV26CompatibilityV0\n\u001b[0;32m      9\u001b[0m __version__ \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m0.2.1\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mNotInstallClass\u001b[39;00m:\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\shimmy\\openai_gym_compatibility.py:35\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mtyping_extensions\u001b[39;00m \u001b[39mimport\u001b[39;00m Protocol, runtime_checkable\n\u001b[0;32m     34\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 35\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mgym\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mgym\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwrappers\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gym\\__init__.py:15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgym\u001b[39;00m \u001b[39mimport\u001b[39;00m logger\n\u001b[0;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgym\u001b[39;00m \u001b[39mimport\u001b[39;00m vector\n\u001b[1;32m---> 15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgym\u001b[39;00m \u001b[39mimport\u001b[39;00m wrappers\n\u001b[0;32m     16\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m     19\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mEnv\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mSpace\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mWrapper\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmake\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mspec\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mregister\u001b[39m\u001b[39m\"\u001b[39m]\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gym\\wrappers\\__init__.py:16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgym\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwrappers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclip_action\u001b[39;00m \u001b[39mimport\u001b[39;00m ClipAction\n\u001b[0;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgym\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwrappers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrecord_episode_statistics\u001b[39;00m \u001b[39mimport\u001b[39;00m RecordEpisodeStatistics\n\u001b[1;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgym\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwrappers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnormalize\u001b[39;00m \u001b[39mimport\u001b[39;00m NormalizeObservation, NormalizeReward\n\u001b[0;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgym\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwrappers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrecord_video\u001b[39;00m \u001b[39mimport\u001b[39;00m RecordVideo, capped_cubic_video_schedule\n\u001b[0;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgym\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwrappers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39morder_enforcing\u001b[39;00m \u001b[39mimport\u001b[39;00m OrderEnforcing\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gym\\wrappers\\normalize.py:43\u001b[0m\n\u001b[0;32m     38\u001b[0m     new_count \u001b[39m=\u001b[39m tot_count\n\u001b[0;32m     40\u001b[0m     \u001b[39mreturn\u001b[39;00m new_mean, new_var, new_count\n\u001b[1;32m---> 43\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mNormalizeObservation\u001b[39;00m(gym\u001b[39m.\u001b[39;49mcore\u001b[39m.\u001b[39mWrapper):\n\u001b[0;32m     44\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m     45\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[0;32m     46\u001b[0m         env,\n\u001b[0;32m     47\u001b[0m         epsilon\u001b[39m=\u001b[39m\u001b[39m1e-8\u001b[39m,\n\u001b[0;32m     48\u001b[0m     ):\n\u001b[0;32m     49\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(env)\n","\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'gym' has no attribute 'core' (most likely due to a circular import)"]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["# deterministic rule\n","# first choose random action\n","# monitor vc and vp while keep going to the same direction\n","# if the sign of the two velocities are different, flip the direction (action)\n","import gymnasium as gym\n","# import gym\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","class Agent:  # Agent base class\n","    def __init__(self):  # agent constructor\n","        self.env = gym.make(\"CartPole-v1\")  # overwritten by the env handed to the agent \n","        self.state = self.env.reset()  #\n","        self.rewards = []  # a list of rewards\n","        self.steps = []  # a sequence of chosen steps\n","        # self.poilcy = []  # list of policy-defined actions at each state (Note that \"policy\" appears misspelled here, look out for the invocation)\n","        self.policy = []\n","        \n","    def select_action(self, state, action):  # returns policy defined action (a') for each state and action pair (s,a). \n","        return action\n","\n","    def play_episode(self, env):  # contains the while loop that drives the agent through the environment\n","        self.env = env  # overwrite the __init__ cartpole default environment\n","        total_reward = 0.0\n","        state = env.reset()\n","        action = random.choice([0,1])  # action list depends on the selected environment\n","        steps = 0\n","        actions = []\n","        isInit = 1\n","        while True:\n","            if isInit:  # set to '1' above\n","                state = state[0]  # start at the first state\n","                isInit = 0  # considered initialized after the first step\n","            action = self.select_action(state, action)\n","            new_state, reward, is_done, _, _ = env.step(action)  # collect new_state, reward, is_done status\n","            total_reward += reward  # \n","            steps +=1\n","            actions.append(action)  # record the action taken\n","            if is_done:  # \n","                break\n","            state = new_state  # updates Agent's self.state member value\n","        return total_reward, steps, actions  # return the final gameplay outcome\n","    \n","    def repeat(self,n_sample):\n","        rewards =[]  # a new set of 'rewards', 'steps', and 'policy' lists is generated for every single round of gameplay \n","        steps=[]\n","        policy = []\n","        for i in range(n_sample):  # forms an effective outer loop by iterating through 'n_sample' runs of agent sgameplay\n","            reward, step, actions = self.play_episode(self.env)  # drive the game_play using the member function above\n","            rewards.append(reward)  # appending each item to the list(s)\n","            steps.append(step)\n","            policy.append(actions)\n","        self.rewards = rewards  # update the agent's rewards list\n","        self.steps = steps\n","        # self.policy = np.array(policy)  # commented out in the original example, turns policy list into an np.array\n","        self.policy = policy\n","        \n","\n","# WE CAN DEFINE DIFFERENT AGENT TYPES AS SHOWN BELOW\n","class ReflexAgent(Agent):  # the user-defined class ReflexAgent, extends Agent\n","    def select_action(self, state, action):  \n","        if state[1]*state[3]<0:\n","            action = int(1-action)  \n","        return action    \n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# \n","def replay(policy):\n","    env = gym.make(\"CartPole-v1\", render_mode='rgb_array')\n","    obs = env.reset()\n","    step = 0\n","    for a in policy:\n","        env.step(a)\n","        show_state(env, step)\n","        step+=1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":282},"colab_type":"code","executionInfo":{"elapsed":10888,"status":"ok","timestamp":1587754491583,"user":{"displayName":"Ketan Ramesh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjfwRjj49F2JlDyKTfXmTOoBNu-A4deHVrB1vg0Qw=s64","userId":"03181076370714837334"},"user_tz":360},"id":"tE7vOWfM77ib","outputId":"9cbdc2a8-a096-4780-baa0-e4b9101f63e5"},"outputs":[],"source":["ra = ReflexAgent()\n","ra.repeat(1000)\n","rewards = ra.rewards\n","plt.hist(rewards,bins=50)\n","print(np.mean(rewards), np.std(rewards), max(rewards))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"Ebh6Jzny77id"},"outputs":[],"source":["class RandomAgent(Agent):       \n","    def select_action(self,state,action):  \n","        return random.choice([0,1]) "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":283},"colab_type":"code","executionInfo":{"elapsed":11422,"status":"ok","timestamp":1587754492129,"user":{"displayName":"Ketan Ramesh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjfwRjj49F2JlDyKTfXmTOoBNu-A4deHVrB1vg0Qw=s64","userId":"03181076370714837334"},"user_tz":360},"id":"4nDxAR1n77ie","outputId":"5f2aed66-1be3-48c2-b01c-d61d2312bcff"},"outputs":[],"source":["rda= RandomAgent()\n","rda.repeat(1000)\n","rewards1 = rda.rewards\n","plt.hist(rewards1,bins=50)\n","print(np.mean(rewards1), np.std(rewards1), max(rewards1))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# best_reflex = ra.policy[np.argmax(rewards)]\n","# replay(best_reflex) \n","# best_random = rda.policy[np.argmax(rewards1)]\n","# replay(best_random) \n","\n","# # Is this a bug?? It didn't terminate; \n","# # it's because everytime the environment is reset, the initial state is different that the policy is not the best for that episode.\n","# # To make it a proper reply you'll need to also save the initial state, or return the entire env object to reproduce the result.\n","# # The goal of your algorithm is that no matter which initial state it started with, it behaves optimally.\n","\n","cart_pole_agent = Agent()  # create an agent class object and extend it with any of the associated class options"]},{"cell_type":"markdown","metadata":{},"source":["## Now let's create special Agent subclasses (extending from the Agent class) to perform in a variety of Gym environments."]},{"cell_type":"markdown","metadata":{},"source":["### Part I: Atari - Assault"]},{"cell_type":"markdown","metadata":{},"source":["Invoke and Define the Atari Gym \"Assault\" Environment"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":511},"colab_type":"code","executionInfo":{"elapsed":30792,"status":"ok","timestamp":1587754511515,"user":{"displayName":"Ketan Ramesh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjfwRjj49F2JlDyKTfXmTOoBNu-A4deHVrB1vg0Qw=s64","userId":"03181076370714837334"},"user_tz":360},"id":"jUaWE78X77ig","outputId":"65fcb88b-3c0c-4896-d490-5616d7723769"},"outputs":[],"source":["# DEFINE THE ENVIRONMENT\n","# Gym documentation: https://www.gymlibrary.dev/environments/atari/assault/\n","\n","# Action Space: Discrete(18)\n","# Observation Space: (210, 160, 3)\n","# Observation High: 255\n","# Observation Low: 0\n","# Import: gym.make(\"ALE/Assault-v5\")\n","# objective: destroy enemies and maximize the agent's time in the game\n","\n","# Actions: [0:NOOP, 1:FIRE, 2:UP, 3:RIGHT, 4:LEFT, 5:RIGHTFIRE, 6:LEFTFIRE]\n","# Observations: Box([0 ... 0], [255 ... 255], (128,), uint8)   # 128 bytes of RAM cionsole space\n","#               Box([[0 ... 0] ... [0  ... 0]], [[255 ... 255] ... [255  ... 255]], (250, 160), uint8)   # grayscale image\n","\n","# IMPORT THE NECESSARY ATARI LIBRARIES\n","import ale_py\n","import shimmy\n","import gymnasium as gym\n","\n","from ale_py import ALEInterface\n","ale = ALEInterface()\n","# env = gym.make(\"ALE/Assault-v5\", render_mode=\"rgb_array\")  # test run - PASSES MUSTER"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# EXTEND THE AGENT CLASS TO DEFINE A SUCCESSFUL AGENT\n","# # https://www.youtube.com/watch?v=fnVIgAGhA08 (DQN Algorithm Refresher)\n","# https://www.tensorflow.org/agents/tutorials/1_dqn_tutorial  (TensorFlow Agent Tutorial)\n","\n","\n","class AtariAssaultAgent:  #\n","    def __init__(self):  # agent constructor\n","        self.env = gym.make(\"ALE/Assault-v5\", render_mode=\"rgb_array\")  # overwritten by the env handed to the agent \n","        self.state = self.env.reset()  #\n","        self.rewards = []  # a list of rewards\n","        self.steps = []  # a sequence of chosen steps\n","        # self.poilcy = []  # list of policy-defined actions at each state (Note that \"policy\" appears misspelled here, look out for the invocation)\n","        self.policy = []\n","        \n","    def select_action(self, state, action):  # returns policy defined action (a') for each state and action pair (s,a). \n","        return action\n","\n","    def play_episode(self, env):  # contains the while loop that drives the agent through the environment\n","        self.env = env  # overwrite the __init__ cartpole default environment\n","        total_reward = 0.0\n","        state = env.reset()\n","        action = random.choice([0,1])  # \n","        steps = 0\n","        actions = []\n","        isInit = 1\n","        while True:\n","            if isInit:  # set to '1' above\n","                state = state[0]  # start at the first state\n","                isInit = 0  # considered initialized after the first step\n","            action = self.select_action(state, action)\n","            new_state, reward, is_done, _, _ = env.step(action)  # collect new_state, reward, is_done status\n","            total_reward += reward  # \n","            steps +=1\n","            actions.append(action)  # record the action taken\n","            if is_done:  # \n","                break\n","            state = new_state  # updates Agent's self.state member value\n","        return total_reward, steps, actions  # return the final gameplay outcome\n","    \n","    def repeat(self,n_sample):\n","        rewards =[]  # a new set of 'rewards', 'steps', and 'policy' lists is generated for every single round of gameplay \n","        steps=[]\n","        policy = []\n","        for i in range(n_sample):  # forms an effective outer loop by iterating through 'n_sample' runs of agent sgameplay\n","            reward, step, actions = self.play_episode(self.env)  # drive the game_play using the member function above\n","            rewards.append(reward)  # appending each item to the list(s)\n","            steps.append(step)\n","            policy.append(actions)\n","        self.rewards = rewards  # update the agent's rewards list\n","        self.steps = steps\n","        # self.policy = np.array(policy)  # commented out in the original example, turns policy list into an np.array\n","        self.policy = policy\n","        \n","\n","# WE CAN DEFINE DIFFERENT AGENT TYPES AS SHOWN BELOW\n","class ClassOne(AtariAssaultAgent):  # the user-defined class ReflexAgent, extends Agent\n","    def select_action(self, state, action):  \n","        if state[1]*state[3]<0:\n","            action = int(1-action)  \n","        return action    \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # begin Python q-learning tutorial: \n","# # the goal is to compose a Q-Table, a memoization table\n","# import gym\n","# import numpy as np\n","# import matplotlib.pyplot as plt\n","\n","# env = gym.make(\"MountainCar-v0\")  # q-learners should perform in other environments also\n","# env.reset()\n","# # print(env.observation_space.high)\n","# # print(env.observation_space.low)\n","# # print(env.action_space.n)\n","\n","# num_wins = 0  # win counter\n","\n","# # define the q-learning parameters (ADJUST THESE)\n","# LEARNING_RATE = 0.15  # 0.1\n","# DISCOUNT = 0.95  # reducing this value increases sensitivity to distant rewards\n","# EPISODES = 10000  # \n","# SHOW_EVERY = 1000  # \n","\n","# DISCRETE_OBS_SIZE = [8] * len(env.observation_space.high)  # number of discrete states in env\n","# discrete_os_win_size = (env.observation_space.high - env.observation_space.low) / DISCRETE_OBS_SIZE  # normalization value that we can use\n","# # print(discrete_os_win_size)\n","\n","# # define the epsilon control parameter\n","# epsilon = 0.5  # controls agent stochasticity, decreases by 'epsilon_decay_value' for the first 'END_EPSILON_DECAYING' episodes\n","# START_EPSILON_DECAYING = 1\n","# DIV_FACTOR = 30  # try 2,3,4,...  larger value == larger delta_epsilon \"chunks\"\n","# END_EPSILON_DECAYING = EPISODES // DIV_FACTOR  # larger divisor => larger delta_epsilon => more exploitation\n","# epsilon_decay_value = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)  # normalized epsilon decay value\n","\n","# q_table = np.random.uniform(low=-2, high=0, size=(DISCRETE_OBS_SIZE + [env.action_space.n]))  # Q-table with states in columns and (state,action) cominations in rows\n","# # print(q_table.shape)\n","# # print(q_table)  # q-table enumerates every possible observation combination\n","\n","# ep_rewards = []  # episode rewards\n","# aggr_ep_rewards = {'ep': [], 'avg': [], 'min': [], 'max': []}  # 'avg' will contain a pooled average\n","\n","\n","# def get_discrete_state(state):\n","#     discrete_state = (state - env.observation_space.low) / discrete_os_win_size  # normalized state value\n","#     return tuple(discrete_state.astype(np.int64))  # returns as a tuple that will eventually comprise the (state,action) pair\n","# # print(discrete_state)\n","# # print(\"initialized q-values:\", q_table[discrete_state])  # prints the q-table associated with 'discrete_state'\n","# # print(np.argmax(q_table[discrete_state]))  # collects the max value among the q-table values in the initialized state\n","\n","\n","# for episode in range(EPISODES):\n","#     episode_reward = 0  # keep track of sequences of events that scored higher\n","#     if episode % SHOW_EVERY == 0:  # give update every SHOW_EVERY episodes\n","#         print(episode)\n","#         render = True\n","#     else: \n","#         render = False\n","        \n","#     discrete_state = get_discrete_state(env.reset())  # collect the 1st discrete state for the new board\n","#     done = False\n","#     while not done:\n","#         # include eploration/exploitation trade-off here\n","#         if np.random.random() <= epsilon:  # if epsilon is larger than the random value\n","#             action = np.random.randint(0, env.action_space.n)  # explore\n","#         else: \n","#             action = np.argmax(q_table[discrete_state])  # exploit\n","            \n","#         new_state, reward, done, _ = env.step(action)  # use it to generate 'new_state', 'reward', and 'done'\n","#         episode_reward += reward  # \n","#         new_discrete_state = get_discrete_state(new_state)  # normalize (AKA 'descretize') the continuous new_state value \n","#         if render:\n","#             env.render()\n","#             # print(reward, new_state)\n","#         if not done: \n","#             max_future_q = np.max(q_table[new_discrete_state])\n","#             current_q = q_table[discrete_state + (action, )]  # \n","#             new_q = ((1-LEARNING_RATE) * current_q) + (LEARNING_RATE * (reward + (DISCOUNT * max_future_q)))  # \n","#             q_table[discrete_state + (action, )] = new_q  # update q value for 'discrete_state' in the q-table\n","#         elif new_state[0] >= env.goal_position:\n","#             print(f\"agent made it to the flag on episode: {episode}\")\n","#             num_wins += 1\n","#             q_table[discrete_state + (action, )] = 0  # assign a \"reward\" of 0 (recall that cost of living is -1)\n","    \n","#         discrete_state = new_discrete_state  # \n","    \n","#     if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING:\n","#         epsilon -= epsilon_decay_value  # update epsilon\n","        \n","#     ep_rewards.append(episode_reward)  # append the episode rewards to the table\n","#     if episode % SHOW_EVERY == 0:\n","#         average_reward = sum(ep_rewards[-SHOW_EVERY:])/len(ep_rewards[-SHOW_EVERY:])\n","#         aggr_ep_rewards['ep'].append(episode)\n","#         aggr_ep_rewards['avg'].append(average_reward)\n","#         aggr_ep_rewards['min'].append(min(ep_rewards[-SHOW_EVERY:]))\n","#         aggr_ep_rewards['max'].append(max(ep_rewards[-SHOW_EVERY:]))\n","        \n","#         print(f\"Episode: {episode}, avg: {average_reward}, min: {min(ep_rewards[-SHOW_EVERY:])}, max: {max(ep_rewards[-SHOW_EVERY:])}\")\n","        \n","# print(f\"Agent won {num_wins} out of {EPISODES} games\")\n","# print(\"epsilon decay value used: \", epsilon_decay_value, \"division factor: \", DIV_FACTOR)\n","# env.close()\n","\n","# plt.plot(aggr_ep_rewards['ep'], aggr_ep_rewards['avg'], label=\"average rewards\")\n","# plt.plot(aggr_ep_rewards['ep'], aggr_ep_rewards['min'], label=\"minimum rewards\")\n","# plt.plot(aggr_ep_rewards['ep'], aggr_ep_rewards['max'], label=\"maximum rewards\")\n","# plt.legend(loc=4)\n","# plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# source: https://www.kaggle.com/code/yaaryan/space-invaders-game-using-deep-q-networks\n","# !pip install tensorflow==1.14\n","import tensorflow as tf\n","tf.__version__\n","\n","import numpy as np\n","import gym\n","from tensorflow.contrib.layers import flatten, conv2d, fully_connected\n","from collections import deque, Counter\n","import random\n","import datetime\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["############## SAVE THIS CODE ##################\n","# THIS CODE SECTION SHOWS ACTIVE PYTHON ENV PATH (Local Windows Env.):\n","# import os \n","# import sys\n","# os.path.dirname(sys.executable)"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"cartpole-edited.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}
